{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "#문장 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "#문장 단위로 토큰화 됨.\n",
    "text = sent_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "#정제, 문장을 단어로 토큰화 \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter \n",
    "\n",
    "vocab = Counter() #파이썬의 counter 모듈을 이용하면 단어의 모든 빈도를 쉽게 계산할 수 있음. \n",
    "\n",
    "sentences = []\n",
    "stop_words = set(stopwords.words('english')) #불용어 모음들 \n",
    "\n",
    "\n",
    "#단어 토큰화 수행 \n",
    "for i in text : \n",
    "    sentence = word_tokenize(i) #모든 토큰화 수행\n",
    "    result = []\n",
    "    \n",
    "    #정수 인코딩 => 자연어 처리 들어가기 전 필수 \n",
    "    for word in sentence :\n",
    "        word = word.lower() #모든 단어를 소문자화하여 단어의 개수를 줄입니다.\n",
    "        if word not in stop_words: #단어 토큰화 된 결과에 대해서 불용어 제거\n",
    "            if len(word) > 2: #단어의 길이가 2이하인 경우에 대하여 추가로 단어 제거 \n",
    "                result.append(word)\n",
    "                vocab[word] = vocab[word]+1 #각 단어의 빈도 count \n",
    "            \n",
    "    sentences.append(result)\n",
    "    \n",
    "    \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "print(vocab) #등장 빈도 순으로 정렬됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(),key=lambda x:x[1],reverse = True) #dictionary => list로 변경 , 많이 나온 순서대로 내림차순 정렬 \n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1:\n",
    "        i = i+1\n",
    "        word_to_index[word] = i \n",
    "        \n",
    "print(word_to_index) #빈도가 높은 순으로 1부터 지정함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot encoding\n",
    "\n",
    "### -국소표현 방법 \n",
    " 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법 \n",
    "단어의 의미, 뉘앙스를 표현할 수 없음\n",
    "ex) One-Hot vector, N-gram, Bag of word, TDM, TF-IDF\n",
    "\n",
    "\n",
    "### - 연속 표현 방법 \n",
    "그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법 \n",
    "단어의 뉘앙스를 표현할 수 있음\n",
    "ex) Word2Vec, (카운트기반)LSA , (예측, 카운트기반) Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot vector : 단어의 빈도수를 카운트해서 단어를 수치화하는 카운트 방법\n",
    "\n",
    "문자를 숫자로 표현하는 기법 중 가장 기본적인 표현 방식\n",
    "단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여 \n",
    "다른 인덱스에는 0 을 부여하는 단어의 벡터 표현\n",
    "\n",
    "#### 방법 \n",
    "#### - 정수 인코딩 : 각 단어에 고유한 인덱스 부여\n",
    "#### - 표현하고 싶은 단어의 인덱스에 1, 다른 단어의 인덱스에는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"이 여름 다시 한번 설레고 싶다 그때 그 여름을 틀어줘 그 여름을 들려줘 이 여름도 언젠가는 그해 여름 오늘이 가장 젊은 내 여름\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '여름', '다시', '한번', '설레고', '싶다', '그때', '그', '여름', '을', '틀어줘', '그', '여름', '을', '들려줘', '이', '여름', '도', '언젠가', '는', '그해', '여름', '오늘이', '가장', '젊은', '내', '여름']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "okt = Okt()\n",
    "\n",
    "token = okt.morphs(example)\n",
    "\n",
    "print(token) #형태소별로 확인 가능함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'이': 0, '여름': 1, '다시': 2, '한번': 3, '설레고': 4, '싶다': 5, '그때': 6, '그': 7, '을': 8, '틀어줘': 9, '들려줘': 10, '도': 11, '언젠가': 12, '는': 13, '그해': 14, '오늘이': 15, '가장': 16, '젊은': 17, '내': 18}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {} \n",
    "\n",
    "for voca in token:\n",
    "    if voca not in word_to_index.keys(): #key로 단어가 없으면 \n",
    "        word_to_index[voca] = len(word_to_index) #\n",
    "        \n",
    "        \n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word,word_to_index): \n",
    "    one_hot_vector = [0]*(len(word_to_index))#단어의 개수대로 index의 개수대로 모두 0이 들어간 벡터 만들어주기 \n",
    "    index = word_to_index[word] #해당하는 단어가 있는 index에 1 넣어줌\n",
    "    one_hot_vector[index] =1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"여름\",word_to_index) #여름이 있는 index 자리에 1 넣어줌. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"가장\",word_to_index) #여름이 있는 index 자리에 1 넣어줌. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 과제 >\n",
    "- 1글자 단어 제외하고 one hot 벡터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= 단어 별로 토큰화====================\n",
      "['이', '여름', '다시', '한번', '설레고', '싶다', '그때', '그', '여름', '을', '틀어줘', '그', '여름', '을', '들려줘', '이', '여름', '도', '언젠가', '는', '그해', '여름', '오늘이', '가장', '젊은', '내', '여름']\n",
      "\n",
      "==========한 개 짜리 단어 제거 ========\n",
      "['여름', '다시', '한번', '설레고', '싶다', '그때', '여름', '틀어줘', '여름', '들려줘', '여름', '언젠가', '그해', '여름', '오늘이', '가장', '젊은', '여름']\n",
      "\n",
      "============단어 dictionary로 변경=========\n",
      "{'이': 0, '여름': 1, '다시': 2, '한번': 3, '설레고': 4, '싶다': 5, '그때': 6, '그': 7, '을': 8, '틀어줘': 9, '들려줘': 10, '도': 11, '언젠가': 12, '는': 13, '그해': 14, '오늘이': 15, '가장': 16, '젊은': 17, '내': 18}\n",
      "\n",
      "===========one hot 인코딩 불러오기=========\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "print(\"============= 단어 별로 토큰화====================\")\n",
    "okt = Okt()\n",
    "token = okt.morphs(example)\n",
    "print(token) #형태소별로 확인 가능함.\n",
    "\n",
    "print(\"\\n==========한 개 짜리 단어 제거 ========\") \n",
    "new_list = []\n",
    "\n",
    "for i in token : \n",
    "    if len(i)>=2 : \n",
    "        new_list.append(i)\n",
    "\n",
    "       \n",
    "print(new_list)\n",
    "\n",
    "print(\"\\n============단어 dictionary로 변경=========\")\n",
    "word_to_index = {} \n",
    "\n",
    "\n",
    "for voca in token:\n",
    "    if voca not in word_to_index.keys(): #key로 단어가 없으면 \n",
    "        word_to_index[voca] = len(word_to_index) #\n",
    "        \n",
    "        \n",
    "print(word_to_index)\n",
    "\n",
    "\n",
    "def one_hot_encoding(word,word_to_index): \n",
    "    one_hot_vector = [0]*(len(word_to_index))#단어의 개수대로 index의 개수대로 모두 0이 들어간 벡터 만들어주기 \n",
    "    index = word_to_index[word] #해당하는 단어가 있는 index에 1 넣어줌\n",
    "    one_hot_vector[index] =1\n",
    "    return one_hot_vector\n",
    "print(\"\\n===========one hot 인코딩 불러오기=========\")\n",
    "print(one_hot_encoding(\"여름\",word_to_index)) #여름이 있는 index 자리에 1 넣어줌. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW\n",
    "#### 기본이 되는 개념이면서 , 텍스트에서 사용된 단어의 종류와 빈도수만을 바탕으로 분석 \n",
    "\n",
    "#### 장점\n",
    "- 전처리가 단순 \n",
    "- 단어들의 빈도를 간단히 수치화할 수 있고, 통계 방법 적용이 가능\n",
    "- 분석 결과의 해석이 용이\n",
    "\n",
    "#### 단점 \n",
    "- 문장 구조를 무시함에 따라 어순 상의 차이 파악 불가능\n",
    "- 동음이의어 구별 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BoW 만들기 \n",
    "- 각 단어에 고유한 Index 부여\n",
    "- 각 인덱스의 위치에 단어 토큰의 등장 횟수(빈도)를 기록한 벡터(Vector) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"이 여름 다시 한번 설레고 싶다. 그때 그 여름을 틀어줘. 그 여름을 들려줘. 이 여름도 언젠가는 그해 여름. 오늘이 가장 젊은 내 여름.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 여름 다시 한번 설레고 싶다 그때 그 여름을 틀어줘 그 여름을 들려줘 이 여름도 언젠가는 그해 여름 오늘이 가장 젊은 내 여름\n"
     ]
    }
   ],
   "source": [
    "#정규 표현식을 통해 온점 제거\n",
    "token = re.sub(\"(\\.)\",\"\",text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '여름', '다시', '한번', '설레고', '싶다', '그때', '그', '여름', '을', '틀어줘', '그', '여름', '을', '들려줘', '이', '여름', '도', '언젠가', '는', '그해', '여름', '오늘이', '가장', '젊은', '내', '여름']\n"
     ]
    }
   ],
   "source": [
    "#토큰화 작업\n",
    "token = okt.morphs(token)\n",
    "\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={} #딕셔너리 생성\n",
    "bow =[] #리스트 \n",
    "\n",
    "for voca in token: #token은 리스트 \n",
    "    if voca not in word2index.keys(): \n",
    "        word2index[voca] = len(word2index)\n",
    "                #token을 읽으면서, word2index에 없는 not in 단어는 새로 추가하고,\n",
    "                #이미 있는 단어는 넘김\n",
    "        bow.insert(len(word2index)-1,1) #Bow 전체에 전부 기본값 1 지정\n",
    "        \n",
    "    else: #이미 있는 단어면(재등장하면) \n",
    "        index = word2index.get(voca) #재등장하는 단어의 인덱스 \n",
    "        bow[index] = bow[index]+1 #재등장한 단어의 해당 인덱스에 1을 더함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'이': 0, '여름': 1, '다시': 2, '한번': 3, '설레고': 4, '싶다': 5, '그때': 6, '그': 7, '을': 8, '틀어줘': 9, '들려줘': 10, '도': 11, '언젠가': 12, '는': 13, '그해': 14, '오늘이': 15, '가장': 16, '젊은': 17, '내': 18}\n",
      "[2, 6, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(word2index)\n",
    "\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#띄어쓰기 기준으로 단어 자르는 token화\n",
    "#영어는 용이하지만 한국어는 조사가 끊어지지 않으므로 사용이 분리함.\n",
    "\n",
    "\n",
    "corpus = ['you know i want your love, because i love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray()) #코퍼스로부터 각 단어의 빈도 수 기록 \n",
    "print(vector.vocabulary_) #각 단어의 인덱스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vector.fit_transform(corpus))\n",
    "#기본으로 길이가 2 이상인 아이만 인식하기 때문에 i는 제외 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1]]\n",
      "{'know': 1, 'want': 3, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "text = [\"you know i want your love. because i love you\"]\n",
    "vect = CountVectorizer(stop_words = [\"you\", \"your\"]) #불용어로 지정하고 제거 (직접지정)\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_) #알파벳 순으로 지정됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1]]\n",
      "{'know': 0, 'want': 2, 'love': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "text = [\"you know i want your love. because i love you\"]\n",
    "vect = CountVectorizer(stop_words = sw)\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
