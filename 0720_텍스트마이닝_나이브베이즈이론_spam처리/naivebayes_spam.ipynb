{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베르누이 나이브베이즈\n",
    "- 데이터 : https://www.kaggle.com/team-ai/spam-text-message-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"spam.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  5572 non-null   object\n",
      " 1   Message   5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  label\n",
       "0      ham  Go until jurong point, crazy.. Available only ...      0\n",
       "1      ham                      Ok lar... Joking wif u oni...      0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...      1\n",
       "3      ham  U dun say so early hor... U c already then say...      0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"] =data[\"Category\"].map({\"spam\":1,\"ham\":0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,) (1672,) (3900,) (1672,)\n"
     ]
    }
   ],
   "source": [
    "X = data[\"Message\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size =0.3,random_state = 103)\n",
    "\n",
    "print(x_train.shape,x_test.shape,y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " #최대 단어 1000개, 토큰화 따로 지정x -> 내장 토큰화는 띄어쓰기 기준\n",
    "# 이진 모델로 지정 binary\n",
    "cv = CountVectorizer(max_features = 1000,binary =True)\n",
    "x_train_cv = cv.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = x_train_cv.toarray()\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['and', 'come', 'down', 'face', 'feel', 'for', 'have', 'heart',\n",
       "        'into', 'life', 'loved', 'making', 'me', 'my', 'on', 'smile',\n",
       "        'sun', 'the', 'you'], dtype='<U15')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '04', '0800', '08000839402', '08000930705', '08712460324', '10', '100', '1000', '10p', '11', '12hrs', '150', '150p', '150ppm', '16', '18', '1st', '20', '200', '2000', '2003', '250', '2day', '2lands', '2nd', '30', '350', '50', '500', '5000', '50p', '750', '800', '8007', '86688', '87066', 'able', 'about', 'abt', 'ac', 'account', 'across', 'actually', 'address', 'aft', 'after', 'afternoon', 'again', 'age', 'age16', 'ago', 'ah', 'aight', 'all', 'almost', 'alone', 'already', 'alright', 'also', 'always', 'am', 'amp', 'an', 'and', 'angry', 'another', 'ans', 'answer', 'any', 'anyone', 'anything', 'anytime', 'anyway', 'apply', 'ard', 'are', 'area', 'around', 'as', 'asap', 'ask', 'askd', 'asked', 'ass', 'at', 'attempt', 'auction', 'available', 'await', 'award', 'awarded', 'away', 'awesome', 'b4', 'babe', 'baby', 'back', 'bad', 'bak', 'balance', 'bank', 'bath', 'bb', 'bcoz', 'be', 'beautiful', 'because', 'bed', 'been', 'before', 'being', 'believe', 'best', 'better', 'between', 'big', 'birthday', 'bit', 'bluetooth', 'bonus', 'book', 'booked', 'bored', 'boss', 'both', 'bout', 'box', 'boy', 'boytoy', 'break', 'bring', 'brother', 'bslvyl', 'bt', 'bus', 'busy', 'but', 'buy', 'by', 'cake', 'call', 'call2optout', 'called', 'caller', 'calling', 'calls', 'camcorder', 'came', 'camera', 'can', 'cant', 'car', 'card', 'care', 'carlos', 'case', 'cash', 'cause', 'cd', 'chance', 'change', 'charge', 'charged', 'chat', 'cheap', 'check', 'checking', 'chennai', 'chikku', 'choose', 'claim', 'class', 'close', 'club', 'co', 'code', 'collect', 'collection', 'college', 'colour', 'com', 'come', 'comes', 'comin', 'coming', 'company', 'complimentary', 'confirm', 'congrats', 'congratulations', 'contact', 'content', 'cool', 'correct', 'cos', 'cost', 'could', 'course', 'coz', 'crave', 'crazy', 'credit', 'cs', 'cum', 'cup', 'currently', 'customer', 'da', 'dad', 'darlin', 'darren', 'dat', 'date', 'dating', 'day', 'days', 'de', 'dear', 'decided', 'decimal', 'deep', 'del', 'delivery', 'den', 'details', 'did', 'didn', 'didnt', 'different', 'difficult', 'dinner', 'direct', 'dis', 'disturb', 'dnt', 'do', 'doctor', 'does', 'doesn', 'doesnt', 'doin', 'doing', 'don', 'done', 'dont', 'double', 'down', 'download', 'draw', 'dream', 'dreams', 'drink', 'drive', 'driving', 'drop', 'drugs', 'dude', 'dun', 'dunno', 'each', 'earlier', 'early', 'easy', 'eat', 'eh', 'either', 'else', 'em', 'email', 'end', 'energy', 'enjoy', 'enough', 'enter', 'entered', 'entry', 'especially', 'eve', 'even', 'evening', 'ever', 'every', 'everyone', 'everything', 'ex', 'exam', 'excellent', 'experience', 'expires', 'eyes', 'face', 'fact', 'family', 'fancy', 'fantastic', 'far', 'fast', 'fat', 'feel', 'feeling', 'felt', 'few', 'film', 'final', 'finally', 'find', 'fine', 'fingers', 'finish', 'finished', 'first', 'fone', 'food', 'for', 'forever', 'forget', 'forgot', 'forwarded', 'found', 'fr', 'free', 'freemsg', 'fri', 'friday', 'friend', 'friends', 'friendship', 'frm', 'frnd', 'frnds', 'from', 'fuck', 'fucking', 'full', 'fun', 'funny', 'game', 'games', 'gas', 'gd', 'ge', 'get', 'gets', 'getting', 'getzed', 'gift', 'girl', 'girls', 'give', 'glad', 'go', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodmorning', 'goodnight', 'got', 'gr8', 'great', 'grins', 'gt', 'guaranteed', 'gud', 'guess', 'guy', 'guys', 'gym', 'ha', 'had', 'haf', 'haha', 'hair', 'half', 'hand', 'happen', 'happened', 'happening', 'happiness', 'happy', 'hard', 'has', 'hav', 'have', 'haven', 'havent', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'hee', 'hello', 'help', 'her', 'here', 'hey', 'hg', 'hi', 'him', 'his', 'hit', 'hiya', 'hmm', 'hmmm', 'hold', 'holiday', 'home', 'hope', 'hoping', 'hot', 'hour', 'hours', 'house', 'how', 'hows', 'http', 'huh', 'hungry', 'hurt', 'ice', 'id', 'identifier', 'if', 'ill', 'im', 'important', 'in', 'inc', 'india', 'info', 'information', 'into', 'invited', 'ipod', 'is', 'isn', 'it', 'its', 'jay', 'job', 'join', 'jus', 'just', 'juz', 'keep', 'kind', 'kinda', 'kiss', 'knew', 'know', 'knw', 'land', 'landline', 'laptop', 'lar', 'last', 'late', 'later', 'latest', 'ldn', 'least', 'leave', 'leaving', 'left', 'leh', 'lei', 'lesson', 'let', 'lets', 'liao', 'library', 'life', 'like', 'line', 'link', 'listen', 'little', 'live', 'll', 'loads', 'loan', 'log', 'lol', 'long', 'look', 'looking', 'lor', 'lose', 'lost', 'lot', 'lots', 'lovable', 'love', 'loved', 'lovely', 'loving', 'lt', 'ltd', 'luck', 'lucky', 'lunch', 'luv', 'made', 'mail', 'make', 'makes', 'making', 'man', 'many', 'march', 'mate', 'mates', 'may', 'mayb', 'maybe', 'me', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'messages', 'met', 'mid', 'midnight', 'might', 'min', 'mind', 'mine', 'mins', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mob', 'mobile', 'mobiles', 'mobileupd8', 'mode', 'mom', 'moment', 'money', 'month', 'months', 'more', 'morning', 'most', 'motorola', 'move', 'movie', 'mp3', 'mr', 'msg', 'msgs', 'mu', 'much', 'mum', 'music', 'must', 'muz', 'my', 'myself', 'na', 'nah', 'name', 'national', 'near', 'need', 'needs', 'net', 'network', 'neva', 'never', 'new', 'news', 'next', 'ni8', 'nice', 'night', 'nite', 'no', 'noe', 'nokia', 'nope', 'normal', 'not', 'nothing', 'now', 'nt', 'number', 'numbers', 'of', 'off', 'offer', 'offers', 'office', 'oh', 'ok', 'okay', 'okie', 'old', 'on', 'once', 'one', 'online', 'only', 'open', 'operator', 'opt', 'or', 'orange', 'orchard', 'order', 'oredi', 'oso', 'other', 'otherwise', 'our', 'out', 'outside', 'over', 'pa', 'pain', 'parents', 'part', 'party', 'pay', 'pc', 'people', 'per', 'person', 'pete', 'phone', 'phones', 'pic', 'pick', 'pics', 'place', 'plan', 'planning', 'plans', 'play', 'player', 'please', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'point', 'points', 'poly', 'poor', 'post', 'pounds', 'pray', 'press', 'pretty', 'price', 'princess', 'private', 'prize', 'prob', 'probably', 'problem', 'project', 'promise', 'pub', 'put', 'question', 'questions', 'quite', 'quiz', 'rakhesh', 'rate', 're', 'reach', 'reached', 'read', 'reading', 'ready', 'real', 'really', 'receive', 'red', 'redeemed', 'remember', 'rental', 'reply', 'replying', 'rest', 'right', 'ring', 'ringtone', 'rite', 'room', 'row', 'run', 'sad', 'sae', 'safe', 'said', 'same', 'sat', 'saturday', 'saw', 'say', 'saying', 'says', 'sch', 'school', 'sea', 'search', 'second', 'secret', 'see', 'seeing', 'selected', 'sell', 'semester', 'send', 'sending', 'sent', 'service', 'services', 'set', 'sexy', 'shall', 'she', 'shit', 'shop', 'shopping', 'short', 'should', 'show', 'shower', 'shows', 'side', 'since', 'sir', 'sis', 'sister', 'sitting', 'sk38xh', 'sleep', 'sleeping', 'slow', 'small', 'smile', 'smiling', 'smoke', 'sms', 'snow', 'so', 'sofa', 'sol', 'some', 'somebody', 'someone', 'something', 'song', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'speak', 'special', 'specially', 'st', 'start', 'started', 'starts', 'statement', 'stay', 'std', 'still', 'stop', 'store', 'story', 'studying', 'stuff', 'stupid', 'suite342', 'sun', 'sunday', 'support', 'supposed', 'sure', 'surprise', 'sweet', 'take', 'takes', 'taking', 'talk', 'talking', 'tc', 'tel', 'tell', 'telling', 'ten', 'terms', 'test', 'text', 'texts', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinkin', 'thinking', 'thinks', 'this', 'thk', 'tho', 'those', 'though', 'thought', 'through', 'tht', 'tickets', 'til', 'till', 'time', 'times', 'tired', 'tmr', 'to', 'today', 'todays', 'together', 'told', 'tomo', 'tomorrow', 'tone', 'tones', 'tonight', 'tonite', 'too', 'took', 'top', 'tot', 'touch', 'tough', 'towards', 'town', 'train', 'treat', 'tried', 'trip', 'true', 'trust', 'try', 'trying', 'ts', 'tv', 'two', 'txt', 'txting', 'txts', 'type', 'ugh', 'uk', 'un', 'understand', 'unless', 'unlimited', 'unsubscribe', 'until', 'up', 'update', 'ur', 'urgent', 'us', 'use', 'used', 'usf', 'valentines', 'valid', 'valued', 've', 'very', 'via', 'video', 'visit', 'voice', 'voucher', 'vouchers', 'w1j6hl', 'wait', 'waiting', 'wake', 'walk', 'wan', 'wanna', 'want', 'wanted', 'wants', 'warm', 'was', 'wasn', 'wat', 'watch', 'watching', 'way', 'we', 'week', 'weekend', 'weekly', 'weeks', 'welcome', 'well', 'wen', 'went', 'were', 'what', 'whatever', 'whats', 'when', 'whenever', 'where', 'which', 'while', 'who', 'whole', 'why', 'wid', 'wif', 'wife', 'wil', 'will', 'win', 'wine', 'winner', 'wish', 'wit', 'with', 'within', 'without', 'wk', 'wkly', 'woke', 'won', 'wonderful', 'wont', 'word', 'words', 'work', 'working', 'world', 'worry', 'worth', 'wot', 'would', 'wow', 'write', 'wrong', 'www', 'xmas', 'xx', 'xxx', 'xy', 'ya', 'yar', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'you', 'your', 'yours', 'yourself', 'yr', 'yup']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 베르누이 나이브베이즈 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf = BernoulliNB()\n",
    "\n",
    "nb_clf.fit(x_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cv = cv.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 840)\t1\n",
      "  (0, 252)\t1\n",
      "  (0, 823)\t1\n",
      "  (0, 832)\t1\n",
      "  (0, 834)\t1\n",
      "  (0, 603)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 318)\t1\n",
      "  (0, 312)\t1\n",
      "  (0, 236)\t1\n",
      "  (0, 773)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 61)\t1\n",
      "  (0, 465)\t1\n",
      "  (0, 589)\t1\n",
      "  (1, 940)\t1\n",
      "  (1, 786)\t1\n",
      "  (1, 615)\t1\n",
      "  (2, 789)\t1\n",
      "  (2, 214)\t1\n",
      "  (2, 859)\t1\n",
      "  (2, 970)\t1\n",
      "  (2, 187)\t1\n",
      "  (2, 858)\t1\n",
      "  (2, 652)\t1\n",
      "  :\t:\n",
      "  (1669, 45)\t1\n",
      "  (1669, 508)\t1\n",
      "  (1669, 434)\t1\n",
      "  (1669, 643)\t1\n",
      "  (1669, 529)\t1\n",
      "  (1669, 237)\t1\n",
      "  (1669, 451)\t1\n",
      "  (1670, 897)\t1\n",
      "  (1670, 610)\t1\n",
      "  (1670, 609)\t1\n",
      "  (1670, 923)\t1\n",
      "  (1670, 273)\t1\n",
      "  (1671, 834)\t1\n",
      "  (1671, 90)\t1\n",
      "  (1671, 422)\t1\n",
      "  (1671, 148)\t1\n",
      "  (1671, 599)\t1\n",
      "  (1671, 532)\t1\n",
      "  (1671, 995)\t1\n",
      "  (1671, 607)\t1\n",
      "  (1671, 957)\t1\n",
      "  (1671, 571)\t1\n",
      "  (1671, 69)\t1\n",
      "  (1671, 138)\t1\n",
      "  (1671, 943)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded2 = x_test_cv.toarray()\n",
    "encoded2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nb_clf.predict(x_test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8086124401913876"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 과제 >\n",
    "- 베르누이 나이브베이즈 분류 모델을 사용하여 스팸 메세지 분류\n",
    "- 정제, 필터링 작업 후 분류하여 성능확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900,) (1672,) (3900,) (1672,)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[array(['and', 'come', 'down', 'face', 'feel', 'for', 'have', 'heart',\n",
      "       'into', 'life', 'loved', 'making', 'me', 'my', 'on', 'smile',\n",
      "       'sun', 'the', 'you'], dtype='<U15')]\n",
      "['000', '04', '0800', '08000839402', '08000930705', '08712460324', '10', '100', '1000', '10p', '11', '12hrs', '150', '150p', '150ppm', '16', '18', '1st', '20', '200', '2000', '2003', '250', '2day', '2lands', '2nd', '30', '350', '50', '500', '5000', '50p', '750', '800', '8007', '86688', '87066', 'able', 'about', 'abt', 'ac', 'account', 'across', 'actually', 'address', 'aft', 'after', 'afternoon', 'again', 'age', 'age16', 'ago', 'ah', 'aight', 'all', 'almost', 'alone', 'already', 'alright', 'also', 'always', 'am', 'amp', 'an', 'and', 'angry', 'another', 'ans', 'answer', 'any', 'anyone', 'anything', 'anytime', 'anyway', 'apply', 'ard', 'are', 'area', 'around', 'as', 'asap', 'ask', 'askd', 'asked', 'ass', 'at', 'attempt', 'auction', 'available', 'await', 'award', 'awarded', 'away', 'awesome', 'b4', 'babe', 'baby', 'back', 'bad', 'bak', 'balance', 'bank', 'bath', 'bb', 'bcoz', 'be', 'beautiful', 'because', 'bed', 'been', 'before', 'being', 'believe', 'best', 'better', 'between', 'big', 'birthday', 'bit', 'bluetooth', 'bonus', 'book', 'booked', 'bored', 'boss', 'both', 'bout', 'box', 'boy', 'boytoy', 'break', 'bring', 'brother', 'bslvyl', 'bt', 'bus', 'busy', 'but', 'buy', 'by', 'cake', 'call', 'call2optout', 'called', 'caller', 'calling', 'calls', 'camcorder', 'came', 'camera', 'can', 'cant', 'car', 'card', 'care', 'carlos', 'case', 'cash', 'cause', 'cd', 'chance', 'change', 'charge', 'charged', 'chat', 'cheap', 'check', 'checking', 'chennai', 'chikku', 'choose', 'claim', 'class', 'close', 'club', 'co', 'code', 'collect', 'collection', 'college', 'colour', 'com', 'come', 'comes', 'comin', 'coming', 'company', 'complimentary', 'confirm', 'congrats', 'congratulations', 'contact', 'content', 'cool', 'correct', 'cos', 'cost', 'could', 'course', 'coz', 'crave', 'crazy', 'credit', 'cs', 'cum', 'cup', 'currently', 'customer', 'da', 'dad', 'darlin', 'darren', 'dat', 'date', 'dating', 'day', 'days', 'de', 'dear', 'decided', 'decimal', 'deep', 'del', 'delivery', 'den', 'details', 'did', 'didn', 'didnt', 'different', 'difficult', 'dinner', 'direct', 'dis', 'disturb', 'dnt', 'do', 'doctor', 'does', 'doesn', 'doesnt', 'doin', 'doing', 'don', 'done', 'dont', 'double', 'down', 'download', 'draw', 'dream', 'dreams', 'drink', 'drive', 'driving', 'drop', 'drugs', 'dude', 'dun', 'dunno', 'each', 'earlier', 'early', 'easy', 'eat', 'eh', 'either', 'else', 'em', 'email', 'end', 'energy', 'enjoy', 'enough', 'enter', 'entered', 'entry', 'especially', 'eve', 'even', 'evening', 'ever', 'every', 'everyone', 'everything', 'ex', 'exam', 'excellent', 'experience', 'expires', 'eyes', 'face', 'fact', 'family', 'fancy', 'fantastic', 'far', 'fast', 'fat', 'feel', 'feeling', 'felt', 'few', 'film', 'final', 'finally', 'find', 'fine', 'fingers', 'finish', 'finished', 'first', 'fone', 'food', 'for', 'forever', 'forget', 'forgot', 'forwarded', 'found', 'fr', 'free', 'freemsg', 'fri', 'friday', 'friend', 'friends', 'friendship', 'frm', 'frnd', 'frnds', 'from', 'fuck', 'fucking', 'full', 'fun', 'funny', 'game', 'games', 'gas', 'gd', 'ge', 'get', 'gets', 'getting', 'getzed', 'gift', 'girl', 'girls', 'give', 'glad', 'go', 'god', 'goes', 'goin', 'going', 'gone', 'gonna', 'good', 'goodmorning', 'goodnight', 'got', 'gr8', 'great', 'grins', 'gt', 'guaranteed', 'gud', 'guess', 'guy', 'guys', 'gym', 'ha', 'had', 'haf', 'haha', 'hair', 'half', 'hand', 'happen', 'happened', 'happening', 'happiness', 'happy', 'hard', 'has', 'hav', 'have', 'haven', 'havent', 'having', 'he', 'head', 'hear', 'heard', 'heart', 'hee', 'hello', 'help', 'her', 'here', 'hey', 'hg', 'hi', 'him', 'his', 'hit', 'hiya', 'hmm', 'hmmm', 'hold', 'holiday', 'home', 'hope', 'hoping', 'hot', 'hour', 'hours', 'house', 'how', 'hows', 'http', 'huh', 'hungry', 'hurt', 'ice', 'id', 'identifier', 'if', 'ill', 'im', 'important', 'in', 'inc', 'india', 'info', 'information', 'into', 'invited', 'ipod', 'is', 'isn', 'it', 'its', 'jay', 'job', 'join', 'jus', 'just', 'juz', 'keep', 'kind', 'kinda', 'kiss', 'knew', 'know', 'knw', 'land', 'landline', 'laptop', 'lar', 'last', 'late', 'later', 'latest', 'ldn', 'least', 'leave', 'leaving', 'left', 'leh', 'lei', 'lesson', 'let', 'lets', 'liao', 'library', 'life', 'like', 'line', 'link', 'listen', 'little', 'live', 'll', 'loads', 'loan', 'log', 'lol', 'long', 'look', 'looking', 'lor', 'lose', 'lost', 'lot', 'lots', 'lovable', 'love', 'loved', 'lovely', 'loving', 'lt', 'ltd', 'luck', 'lucky', 'lunch', 'luv', 'made', 'mail', 'make', 'makes', 'making', 'man', 'many', 'march', 'mate', 'mates', 'may', 'mayb', 'maybe', 'me', 'mean', 'means', 'meant', 'meet', 'meeting', 'message', 'messages', 'met', 'mid', 'midnight', 'might', 'min', 'mind', 'mine', 'mins', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mob', 'mobile', 'mobiles', 'mobileupd8', 'mode', 'mom', 'moment', 'money', 'month', 'months', 'more', 'morning', 'most', 'motorola', 'move', 'movie', 'mp3', 'mr', 'msg', 'msgs', 'mu', 'much', 'mum', 'music', 'must', 'muz', 'my', 'myself', 'na', 'nah', 'name', 'national', 'near', 'need', 'needs', 'net', 'network', 'neva', 'never', 'new', 'news', 'next', 'ni8', 'nice', 'night', 'nite', 'no', 'noe', 'nokia', 'nope', 'normal', 'not', 'nothing', 'now', 'nt', 'number', 'numbers', 'of', 'off', 'offer', 'offers', 'office', 'oh', 'ok', 'okay', 'okie', 'old', 'on', 'once', 'one', 'online', 'only', 'open', 'operator', 'opt', 'or', 'orange', 'orchard', 'order', 'oredi', 'oso', 'other', 'otherwise', 'our', 'out', 'outside', 'over', 'pa', 'pain', 'parents', 'part', 'party', 'pay', 'pc', 'people', 'per', 'person', 'pete', 'phone', 'phones', 'pic', 'pick', 'pics', 'place', 'plan', 'planning', 'plans', 'play', 'player', 'please', 'pls', 'plus', 'plz', 'pm', 'po', 'pobox', 'point', 'points', 'poly', 'poor', 'post', 'pounds', 'pray', 'press', 'pretty', 'price', 'princess', 'private', 'prize', 'prob', 'probably', 'problem', 'project', 'promise', 'pub', 'put', 'question', 'questions', 'quite', 'quiz', 'rakhesh', 'rate', 're', 'reach', 'reached', 'read', 'reading', 'ready', 'real', 'really', 'receive', 'red', 'redeemed', 'remember', 'rental', 'reply', 'replying', 'rest', 'right', 'ring', 'ringtone', 'rite', 'room', 'row', 'run', 'sad', 'sae', 'safe', 'said', 'same', 'sat', 'saturday', 'saw', 'say', 'saying', 'says', 'sch', 'school', 'sea', 'search', 'second', 'secret', 'see', 'seeing', 'selected', 'sell', 'semester', 'send', 'sending', 'sent', 'service', 'services', 'set', 'sexy', 'shall', 'she', 'shit', 'shop', 'shopping', 'short', 'should', 'show', 'shower', 'shows', 'side', 'since', 'sir', 'sis', 'sister', 'sitting', 'sk38xh', 'sleep', 'sleeping', 'slow', 'small', 'smile', 'smiling', 'smoke', 'sms', 'snow', 'so', 'sofa', 'sol', 'some', 'somebody', 'someone', 'something', 'song', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'speak', 'special', 'specially', 'st', 'start', 'started', 'starts', 'statement', 'stay', 'std', 'still', 'stop', 'store', 'story', 'studying', 'stuff', 'stupid', 'suite342', 'sun', 'sunday', 'support', 'supposed', 'sure', 'surprise', 'sweet', 'take', 'takes', 'taking', 'talk', 'talking', 'tc', 'tel', 'tell', 'telling', 'ten', 'terms', 'test', 'text', 'texts', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'them', 'then', 'there', 'these', 'they', 'thing', 'things', 'think', 'thinkin', 'thinking', 'thinks', 'this', 'thk', 'tho', 'those', 'though', 'thought', 'through', 'tht', 'tickets', 'til', 'till', 'time', 'times', 'tired', 'tmr', 'to', 'today', 'todays', 'together', 'told', 'tomo', 'tomorrow', 'tone', 'tones', 'tonight', 'tonite', 'too', 'took', 'top', 'tot', 'touch', 'tough', 'towards', 'town', 'train', 'treat', 'tried', 'trip', 'true', 'trust', 'try', 'trying', 'ts', 'tv', 'two', 'txt', 'txting', 'txts', 'type', 'ugh', 'uk', 'un', 'understand', 'unless', 'unlimited', 'unsubscribe', 'until', 'up', 'update', 'ur', 'urgent', 'us', 'use', 'used', 'usf', 'valentines', 'valid', 'valued', 've', 'very', 'via', 'video', 'visit', 'voice', 'voucher', 'vouchers', 'w1j6hl', 'wait', 'waiting', 'wake', 'walk', 'wan', 'wanna', 'want', 'wanted', 'wants', 'warm', 'was', 'wasn', 'wat', 'watch', 'watching', 'way', 'we', 'week', 'weekend', 'weekly', 'weeks', 'welcome', 'well', 'wen', 'went', 'were', 'what', 'whatever', 'whats', 'when', 'whenever', 'where', 'which', 'while', 'who', 'whole', 'why', 'wid', 'wif', 'wife', 'wil', 'will', 'win', 'wine', 'winner', 'wish', 'wit', 'with', 'within', 'without', 'wk', 'wkly', 'woke', 'won', 'wonderful', 'wont', 'word', 'words', 'work', 'working', 'world', 'worry', 'worth', 'wot', 'would', 'wow', 'write', 'wrong', 'www', 'xmas', 'xx', 'xxx', 'xy', 'ya', 'yar', 'yeah', 'year', 'years', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'you', 'your', 'yours', 'yourself', 'yr', 'yup']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rkwjs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rkwjs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-9e402b1e5608>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m##########데이터 전처리\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mx_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_english_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#텍스트 정제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-9e402b1e5608>\u001b[0m in \u001b[0;36mclean_english_documents\u001b[1;34m(documents)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#불용어 제거\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m                 \u001b[0mclean_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def clean_english_documents(documents):\n",
    "    #텍스트 정제 (HTML 태그 제거)\n",
    "    for i, document in enumerate(documents):\n",
    "        document = BeautifulSoup(document, 'html.parser').text \n",
    "        documents[i] = document\n",
    "\n",
    "    #텍스트 정제 (특수기호 제거)\n",
    "    for i, document in enumerate(documents):\n",
    "        document = re.sub(r'[^ A-Za-z]', '', document) #특수기호 제거, 정규 표현식 \n",
    "        documents[i] = document\n",
    "\n",
    "    #텍스트 정제 (불용어 제거)\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    for i, document in enumerate(documents):\n",
    "        clean_words = []\n",
    "        for word in nltk.tokenize.word_tokenize(document):\n",
    "            word = word.lower()\n",
    "            if word not in nltk.corpus.stopwords.words('english'): #불용어 제거\n",
    "                clean_words.append(word)\n",
    "        document = ' '.join(clean_words)\n",
    "        documents[i] = document\n",
    "\n",
    "    #텍스트 정제 (형태소 분석)\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    for i, document in enumerate(documents):\n",
    "        clean_words = []\n",
    "        for word in nltk.tag.pos_tag(nltk.tokenize.word_tokenize(document)):\n",
    "            if word[1] in ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']: #명사, 동사, 형용사\n",
    "                clean_words.append(word[0])\n",
    "        document = ' '.join(clean_words)\n",
    "        document = document.lower()\n",
    "        documents[i] = document\n",
    "\n",
    "    #텍스트 정제 (어간 추출)\n",
    "    for i, document in enumerate(documents):\n",
    "        clean_words = []\n",
    "        for word in nltk.tokenize.word_tokenize(document):\n",
    "            word = word.lower()\n",
    "            stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "            word = stemmer.stem(word) #어간 추출\n",
    "            clean_words.append(word)\n",
    "        document = ' '.join(clean_words)\n",
    "        documents[i] = document\n",
    "\n",
    "    return documents\n",
    "\n",
    "##########데이터 로드\n",
    "data = pd.read_csv(\"spam.csv\")\n",
    "data.head()\n",
    "\n",
    "data[\"label\"] =data[\"Category\"].map({\"spam\":1,\"ham\":0})\n",
    "data.head()\n",
    "\n",
    "X = data[\"Message\"]\n",
    "y = data[\"label\"]\n",
    "\n",
    "      \n",
    "      \n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size =0.3,random_state = 103)\n",
    "print(x_train.shape,x_test.shape,y_train.shape, y_test.shape)\n",
    "\n",
    "#최대 단어 1000개, 토큰화 따로 지정x -> 내장 토큰화는 띄어쓰기 기준\n",
    "# 이진 모델로 지정 binary\n",
    "cv = CountVectorizer(max_features = 1000,binary =True)\n",
    "x_train_cv = cv.fit_transform(x_train)\n",
    "\n",
    "encoded2 = x_test_cv.toarray()\n",
    "print(encoded2)\n",
    "print(cv.inverse_transform(encoded[0]))\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "\n",
    "pred = nb_clf.predict(x_test_cv)\n",
    "\n",
    "x_data = np.array(X)\n",
    "y_data = np.array(y)\n",
    "\n",
    "labels = ['1', '0']\n",
    "\n",
    "##########데이터 분석\n",
    "\n",
    "##########데이터 전처리\n",
    "\n",
    "x_data = clean_english_documents(x_data) #텍스트 정제\n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(x_data)\n",
    "x_data = transformer.transform(x_data) #단어 카운트 가중치\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777, stratify=y_data)\n",
    "\n",
    "##########모델 생성\n",
    "\n",
    "model = BernoulliNB(alpha=1.0)\n",
    "\n",
    "##########모델 학습\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "##########모델 검증\n",
    "\n",
    "print(model.score(x_test, y_test)) #1.0\n",
    "\n",
    "##########모델 예측\n",
    "\n",
    "x_test = x\n",
    "x_test = clean_english_documents(x_test) #텍스트 정제\n",
    "x_test = transformer.transform(x_test) #단어 카운트 가중치\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "label = labels[y_predict[0]]\n",
    "y_predict = model.predict_proba(x_test)\n",
    "confidence = y_predict[0][y_predict[0].argmax()]\n",
    "\n",
    "print(label, confidence) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
